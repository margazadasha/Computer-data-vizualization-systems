{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":4532039,"sourceType":"datasetVersion","datasetId":2579480}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/dariamargaza/4-4-6?scriptVersionId=234017383\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"**Лабораторна робота №4**\n\n**Тема:** Побудова моделі для детекції зображень з датасету Cards image за допомогою згортки\n\n**Завдання.** Побудувати модель для датасета cards.csv за допомогою згортки","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision import datasets, transforms, models\nfrom torch.utils.data import DataLoader, random_split\nfrom torchvision.utils import make_grid\nimport torch.nn.functional as F\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.metrics import confusion_matrix, f1_score\nimport seaborn as sns\n\nimport warnings\n# filter warnings\nwarnings.filterwarnings('ignore')\n\nfrom tqdm import tqdm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T09:37:29.614598Z","iopub.execute_input":"2025-04-15T09:37:29.615297Z","iopub.status.idle":"2025-04-15T09:37:29.619682Z","shell.execute_reply.started":"2025-04-15T09:37:29.615272Z","shell.execute_reply":"2025-04-15T09:37:29.618939Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_dir = '/kaggle/input/cards-image-datasetclassification/train'\nvalid_dir = '/kaggle/input/cards-image-datasetclassification/valid'\ntest_dir ='/kaggle/input/cards-image-datasetclassification/test'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T09:38:33.410628Z","iopub.execute_input":"2025-04-15T09:38:33.41129Z","iopub.status.idle":"2025-04-15T09:38:33.41475Z","shell.execute_reply.started":"2025-04-15T09:38:33.411265Z","shell.execute_reply":"2025-04-15T09:38:33.413938Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"transform = transforms.Compose([\n    transforms.Resize((150, 150)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T09:38:52.761074Z","iopub.execute_input":"2025-04-15T09:38:52.761344Z","iopub.status.idle":"2025-04-15T09:38:52.765526Z","shell.execute_reply.started":"2025-04-15T09:38:52.761324Z","shell.execute_reply":"2025-04-15T09:38:52.764771Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_dataset = datasets.ImageFolder(train_dir, transform=transform)\ntest_dataset = datasets.ImageFolder(test_dir, transform=transform)\nvalid_dataset = datasets.ImageFolder(valid_dir, transform=transform)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T09:39:27.932708Z","iopub.execute_input":"2025-04-15T09:39:27.933012Z","iopub.status.idle":"2025-04-15T09:39:38.758325Z","shell.execute_reply.started":"2025-04-15T09:39:27.93299Z","shell.execute_reply":"2025-04-15T09:39:38.757575Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\nvalid_loader = DataLoader(valid_dataset, batch_size=32)\ntest_loader = DataLoader(test_dataset, batch_size=32)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T09:40:02.559257Z","iopub.execute_input":"2025-04-15T09:40:02.559715Z","iopub.status.idle":"2025-04-15T09:40:02.563853Z","shell.execute_reply.started":"2025-04-15T09:40:02.559693Z","shell.execute_reply":"2025-04-15T09:40:02.563204Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def imshow(images, labels, class_names):\n    images = images.numpy().transpose((1, 2, 0))\n    mean = np.array([0.485, 0.456, 0.406])\n    std = np.array([0.229, 0.224, 0.225])\n    images = std * images + mean\n    images = np.clip(images, 0, 1)\n    plt.imshow(images)\n    plt.title(\", \".join([class_names[l] for l in labels]))\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T09:40:17.816159Z","iopub.execute_input":"2025-04-15T09:40:17.816637Z","iopub.status.idle":"2025-04-15T09:40:17.8211Z","shell.execute_reply.started":"2025-04-15T09:40:17.816612Z","shell.execute_reply":"2025-04-15T09:40:17.820496Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class_names = train_loader.dataset.classes","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T09:40:31.05286Z","iopub.execute_input":"2025-04-15T09:40:31.053427Z","iopub.status.idle":"2025-04-15T09:40:31.056396Z","shell.execute_reply.started":"2025-04-15T09:40:31.053406Z","shell.execute_reply":"2025-04-15T09:40:31.055871Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Виведення 10 зображень із тренувального набору\ndataiter = iter(train_loader)\nimages, labels = next(dataiter)\n\n# Відображення зображень\nimshow(make_grid(images[:10], nrow=5), labels[:10], class_names)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T09:41:01.922073Z","iopub.execute_input":"2025-04-15T09:41:01.922659Z","iopub.status.idle":"2025-04-15T09:41:02.585276Z","shell.execute_reply.started":"2025-04-15T09:41:01.922635Z","shell.execute_reply":"2025-04-15T09:41:02.584542Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(f\"Кількість класів: {len(train_dataset.classes)}\")\nprint(f\"Класи: {train_dataset.classes}\")\nprint(f\"Розмір тренувального набору: {len(train_dataset)}\")\nprint(f\"Розмір валідаційного набору: {len(valid_dataset)}\")\nprint(f\"Розмір тестового набору: {len(test_dataset)}\")\n\n# Перевірка балансу класів у тренувальному наборі\nclass_counts = {class_name: 0 for class_name in train_dataset.classes}\nfor _, label in train_dataset.samples:\n    class_counts[train_dataset.classes[label]] += 1\n\nprint(\"\\nРозподіл класів у тренувальному наборі:\")\nfor class_name, count in class_counts.items():\n    print(f\"{class_name}: {count}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T09:41:43.367784Z","iopub.execute_input":"2025-04-15T09:41:43.368104Z","iopub.status.idle":"2025-04-15T09:41:43.375198Z","shell.execute_reply.started":"2025-04-15T09:41:43.368081Z","shell.execute_reply":"2025-04-15T09:41:43.374492Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass SimpleCNN(nn.Module):\n    def __init__(self, num_classes=53):\n        super(SimpleCNN, self).__init__()\n        \n        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1)\n        self.bn1 = nn.BatchNorm2d(32)\n        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n        \n        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n        self.bn2 = nn.BatchNorm2d(64)\n        \n        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n        self.bn3 = nn.BatchNorm2d(128)\n        \n        # Обчислити розмір вхідних даних для повнозв'язного шару\n        self._initialize_fc_layer()\n        \n        self.fc1 = nn.Linear(self.fc1_in_features, 512)\n        self.fc2 = nn.Linear(512, num_classes)\n    \n    def _initialize_fc_layer(self):\n        # Прокинемо зображення через конволюційні шари, щоб дізнатися розмір\n        with torch.no_grad():\n            dummy_input = torch.zeros(1, 3, 150, 150)\n            dummy_output = self._forward_conv_layers(dummy_input)\n            self.fc1_in_features = dummy_output.numel()\n\n    def _forward_conv_layers(self, x):\n        x = self.pool(F.relu(self.bn1(self.conv1(x))))\n        x = self.pool(F.relu(self.bn2(self.conv2(x))))\n        x = self.pool(F.relu(self.bn3(self.conv3(x))))\n        return x\n\n    def forward(self, x):\n        x = self._forward_conv_layers(x)\n        x = x.view(-1, self.fc1_in_features)\n        x = F.relu(self.fc1(x))\n        x = self.fc2(x)\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T09:50:12.845099Z","iopub.execute_input":"2025-04-15T09:50:12.845375Z","iopub.status.idle":"2025-04-15T09:50:12.853376Z","shell.execute_reply.started":"2025-04-15T09:50:12.845355Z","shell.execute_reply":"2025-04-15T09:50:12.852706Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = SimpleCNN(num_classes=53)\n\n# Перенесення моделі на GPU, якщо доступно\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T09:50:20.421633Z","iopub.execute_input":"2025-04-15T09:50:20.421926Z","iopub.status.idle":"2025-04-15T09:50:20.641981Z","shell.execute_reply.started":"2025-04-15T09:50:20.421899Z","shell.execute_reply":"2025-04-15T09:50:20.641217Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"criterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\n# Налаштування тренувального процесу\nnum_epochs = 7\nbest_val_loss = float('inf')\ntrain_losses = []\ntrain_accuracies = []\nvalid_losses = []\nvalid_accuracies = []","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T09:59:58.163087Z","iopub.execute_input":"2025-04-15T09:59:58.163827Z","iopub.status.idle":"2025-04-15T09:59:58.169653Z","shell.execute_reply.started":"2025-04-15T09:59:58.163781Z","shell.execute_reply":"2025-04-15T09:59:58.168859Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for epoch in range(num_epochs):\n    model.train()\n    running_loss = 0.0\n    correct = 0\n    total = 0\n\n    for inputs, labels in tqdm(train_loader):\n        inputs, labels = inputs.to(device), labels.to(device)\n\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item()\n        \n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\n    train_loss = running_loss / len(train_loader)\n    train_accuracy = correct / total\n    train_losses.append(train_loss)\n    train_accuracies.append(train_accuracy)\n\n    # Валідація\n    model.eval()\n    valid_loss = 0.0\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for inputs, labels in valid_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            valid_loss += loss.item()\n            \n            _, predicted = torch.max(outputs.data, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n\n    valid_loss /= len(valid_loader)\n    valid_accuracy = correct / total\n    valid_losses.append(valid_loss)\n    valid_accuracies.append(valid_accuracy)\n\n    print(f'Epoch {epoch+1}/{num_epochs}:')\n    print(f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}')\n    print(f'Valid Loss: {valid_loss:.4f}, Valid Accuracy: {valid_accuracy:.4f}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T10:00:01.434781Z","iopub.execute_input":"2025-04-15T10:00:01.435061Z","iopub.status.idle":"2025-04-15T10:02:44.17899Z","shell.execute_reply.started":"2025-04-15T10:00:01.43504Z","shell.execute_reply":"2025-04-15T10:02:44.178351Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def visualize_predictions(model, dataloader, class_names, num_images=5):\n    was_training = model.training\n    model.eval()\n    images_so_far = 0\n    fig = plt.figure(figsize=(15, 10))\n\n    with torch.no_grad():\n        for inputs, labels in dataloader:\n            inputs = inputs.to(device)\n            labels = labels.to(device)\n\n            outputs = model(inputs)\n            _, preds = torch.max(outputs, 1)\n\n            for j in range(inputs.size(0)):\n                if images_so_far == num_images:\n                    model.train(mode=was_training)\n                    plt.tight_layout()\n                    plt.show()\n                    return\n\n                images_so_far += 1\n                ax = plt.subplot(num_images//2 + 1, 2, images_so_far)\n                ax.axis('off')\n                ax.set_title(f'Predicted: {class_names[preds[j]]}\\nTrue: {class_names[labels[j]]}')\n\n                # Денормалізація зображення для відображення\n                inv_normalize = transforms.Normalize(\n                    mean=[-0.485/0.229, -0.456/0.224, -0.406/0.225],\n                    std=[1/0.229, 1/0.224, 1/0.225]\n                )\n                inp = inv_normalize(inputs.cpu()[j]).numpy().transpose((1, 2, 0))\n                inp = np.clip(inp, 0, 1)\n\n                plt.imshow(inp)\n\n    model.train(mode=was_training)\n    plt.tight_layout()\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T10:04:48.033947Z","iopub.execute_input":"2025-04-15T10:04:48.034853Z","iopub.status.idle":"2025-04-15T10:04:48.041596Z","shell.execute_reply.started":"2025-04-15T10:04:48.034826Z","shell.execute_reply":"2025-04-15T10:04:48.040932Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def plot_training_metrics(train_losses, train_accuracies, valid_losses, valid_accuracies):\n    epochs = range(1, len(train_losses) + 1)\n\n    plt.figure(figsize=(12, 5))\n\n    # Plot loss\n    plt.subplot(1, 2, 1)\n    plt.plot(epochs, train_losses, 'b-', label='Training Loss')\n    plt.plot(epochs, valid_losses, 'r-', label='Validation Loss')\n    plt.title('Training and Validation Loss')\n    plt.xlabel('Epochs')\n    plt.ylabel('Loss')\n    plt.legend()\n\n    # Plot accuracy\n    plt.subplot(1, 2, 2)\n    plt.plot(epochs, train_accuracies, 'b-', label='Training Accuracy')\n    plt.plot(epochs, valid_accuracies, 'r-', label='Validation Accuracy')\n    plt.title('Training and Validation Accuracy')\n    plt.xlabel('Epochs')\n    plt.ylabel('Accuracy')\n    plt.legend()\n\n    plt.tight_layout()\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T10:06:04.754953Z","iopub.execute_input":"2025-04-15T10:06:04.755473Z","iopub.status.idle":"2025-04-15T10:06:04.760843Z","shell.execute_reply.started":"2025-04-15T10:06:04.755449Z","shell.execute_reply":"2025-04-15T10:06:04.760181Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"all_preds = []\nall_labels = []\ncorrect = 0\ntotal = 0\n\nmodel.eval()\nwith torch.no_grad():\n    for images, labels in test_loader:\n        images = images.to(device)\n        labels = labels.to(device)\n\n        outputs = model(images)\n        _, predicted = torch.max(outputs.data, 1)\n\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\n        all_preds.extend(predicted.cpu().numpy())\n        all_labels.extend(labels.cpu().numpy())\n\nclass_names = train_dataset.classes \n\n# Обчислення точності та F1 \ntest_accuracy = correct / total\nf1 = f1_score(all_labels, all_preds, average='weighted')\n\nprint(f'Test Accuracy: {test_accuracy:.4f}')\nprint(f'F1 Score: {f1:.4f}')\n\n# Створення матриці помилок\ncm = confusion_matrix(all_labels, all_preds)\nplt.figure(figsize=(10,8))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n            xticklabels=class_names, yticklabels=class_names)\nplt.title('Confusion Matrix')\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T10:11:13.233761Z","iopub.execute_input":"2025-04-15T10:11:13.234129Z","iopub.status.idle":"2025-04-15T10:11:18.547072Z","shell.execute_reply.started":"2025-04-15T10:11:13.234106Z","shell.execute_reply":"2025-04-15T10:11:18.546211Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plot_training_metrics(train_losses, train_accuracies, valid_losses, valid_accuracies)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T10:13:16.16141Z","iopub.execute_input":"2025-04-15T10:13:16.161689Z","iopub.status.idle":"2025-04-15T10:13:16.534351Z","shell.execute_reply.started":"2025-04-15T10:13:16.161667Z","shell.execute_reply":"2025-04-15T10:13:16.533545Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"visualize_predictions(model, test_loader, train_dataset.classes, 10)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T10:13:51.015624Z","iopub.execute_input":"2025-04-15T10:13:51.016351Z","iopub.status.idle":"2025-04-15T10:13:51.611682Z","shell.execute_reply.started":"2025-04-15T10:13:51.016324Z","shell.execute_reply":"2025-04-15T10:13:51.610986Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Побудова перетренованої моделі**","metadata":{}},{"cell_type":"code","source":"model = models.vgg16(pretrained=True)\n\nfor param in model.parameters():\n    param.requires_grad = False\n\nfor param in model.features[24:].parameters():\n    param.requires_grad = True\n\nnum_ftrs = model.classifier[6].in_features\nmodel.classifier[6] = nn.Linear(num_ftrs, 53)\n\nnum_ftrs = model.classifier[6].in_features\nmodel.classifier[6] = nn.Linear(num_ftrs, 53)\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T11:08:26.818225Z","iopub.execute_input":"2025-04-15T11:08:26.818504Z","iopub.status.idle":"2025-04-15T11:08:28.526852Z","shell.execute_reply.started":"2025-04-15T11:08:26.818482Z","shell.execute_reply":"2025-04-15T11:08:28.526312Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"criterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=0.0001)\n\nnum_epochs = 10\nbest_valid_loss = float('inf')\ntrain_losses = []\ntrain_accuracies = []\nvalid_losses = []\nvalid_accuracies = []","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T11:20:52.745712Z","iopub.execute_input":"2025-04-15T11:20:52.746419Z","iopub.status.idle":"2025-04-15T11:20:52.751102Z","shell.execute_reply.started":"2025-04-15T11:20:52.746393Z","shell.execute_reply":"2025-04-15T11:20:52.75054Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for epoch in range(num_epochs):\n    model.train()\n    running_loss = 0.0\n    correct = 0\n    total = 0\n\n    for inputs, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Training\"):\n        inputs, labels = inputs.to(device), labels.to(device)\n\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item()\n        \n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\n    train_loss = running_loss / len(train_loader)\n    train_accuracy = correct / total\n    train_losses.append(train_loss)\n    train_accuracies.append(train_accuracy)\n\n    # Валідація\n    model.eval()\n    valid_loss = 0.0\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for inputs, labels in tqdm(valid_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Validation\"):\n            inputs, labels = inputs.to(device), labels.to(device)\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            valid_loss += loss.item()\n            \n            _, predicted = torch.max(outputs.data, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n\n    valid_loss /= len(valid_loader)\n    valid_accuracy = correct / total\n    valid_losses.append(valid_loss)\n    valid_accuracies.append(valid_accuracy)\n\n    print(f'Epoch {epoch+1}/{num_epochs}:')\n    print(f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}')\n    print(f'Valid Loss: {valid_loss:.4f}, Valid Accuracy: {valid_accuracy:.4f}')\n\n    if valid_loss < best_valid_loss:\n        best_valid_loss = valid_loss\n        torch.save(model.state_dict(), 'best_model_1.pth')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T11:20:55.36419Z","iopub.execute_input":"2025-04-15T11:20:55.364749Z","iopub.status.idle":"2025-04-15T11:25:59.135329Z","shell.execute_reply.started":"2025-04-15T11:20:55.364722Z","shell.execute_reply":"2025-04-15T11:25:59.134404Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Висновок:** В ході виконання даної лабораторної роботи було реалізовано дві моделі для класифікації зображень із датасету Cards image: власну згорткову нейронну мережу (CNN), побудовану з нуля, та претреновану модель VGG16, адаптовану до задачі за допомогою transfer learning.\n\nВласна модель CNN досягла точності 86% на валідаційній вибірці. Це свідчить про її здатність успішно навчатися на наявному датасеті та виокремлювати важливі ознаки зображень. Проте після застосування моделі VGG16, попередньо навченої на великому датасеті ImageNet, точність класифікації зросла до 90%.\n\nОтримані результати підтверджують ефективність підходу transfer learning, особливо в задачах з обмеженим обсягом навчальних даних. Завдяки наявності великої кількості вже вивчених ознак, VGG16 змогла краще узагальнювати та розпізнавати класи зображень, ніж модель, навчена з нуля.\n\nТаким чином, хоча власна CNN показала доволі високу продуктивність, використання претренованої моделі VGG16 дозволило досягти ще вищих результатів. У практичних задачах це підтверджує доцільність застосування глибоких попередньо навчених моделей як базового інструменту для розв’язання подібних проблем.","metadata":{}}]}